{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#knowledge-graphs-llms-harnessing-large-language-models-with-neo4j","title":"Knowledge Graphs &amp; LLMs: Harnessing Large Language Models with Neo4j","text":"<p>This is the first blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in blog posts. You can read the later blog posts here:</p> <ul> <li>Fine-Tuning vs Retrieval-Augmented Generation</li> <li>Multi-Hop Question Answering</li> </ul> <p>Large Language Models (LLMs) like ChatGPT have taken the world by storm in 2023 due to their ability to understand and generate human-like text. Their capacity to adapt to different conversational contexts, answer questions across a wide range of topics, and even simulate creative writing has revolutionized the way humans and machines interact, sparking a new wave of artificial intelligence applications.</p> <p></p> <p>Steampunk computer wall with a magic mirror operated by ants running in transparent tubes (Midjourney)</p> <p>Thanks to their ability to \u201cunderstand\u201d, generate, and refine human-like text, LLMs offer us new methods for working with data. Our team at Neo4j has started a project to explore, develop, _and showcase_ practical uses of these LLMs in conjunction with Neo4j.</p> <p>One key aspect of this project is the integration of graph database technology and concepts into the LLM application stack. By doing so, we expect to enhance the accuracy, transparency, _and predictability_ of the model output and open up new use-cases both for using LLMs as well as databases.</p> <p>As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute.</p> <p>_While we base our project on our current understanding and technology, we fully acknowledge that this is a rapidly advancing field, and future findings may refine our approach. Consequently, our perspective and strategies are subject to change in response to new data and technological progress.</p>"},{"location":"#identifying-the-real-world-use-cases","title":"Identifying the Real-World Use Cases","text":"<p>The initiation phase of our project focused on the identification of real-world use cases, which would form the basis for our upcoming solutions. After thorough research, market analysis, and customer interactions, we\u2019ve narrowed down two initial use cases that frequently feature in our conversations with customers.</p>"},{"location":"#1-natural-language-interface-to-a-knowledge-graph","title":"1. Natural Language Interface to a Knowledge Graph","text":"<p>Our first use case focuses on developing a natural language interface for knowledge graphs. The goal is to create a user interface that simplifies the process of data selection, querying and processing, making data more accessible and easier to understand.</p> <p>Allowing you to \u201ctalk to your database\u201d.</p> <p></p> <p>Midjourney imagination of a natural language interface for a KG</p> <p>The preferred method for this is a chat-like interface that would generate database queries based on the user question and the inferred schema of the database.</p> <p>Based on feedback from our users, there is a significant demand for natural language responses over just citing data and linking to sources. By utilizing LLMs, we can provide these responses, presenting information in a way that mimics natural, human conversation.</p> <p>We\u2019re exploring techniques to inform the LLMs about the content of the knowledge graph. This could involve a similarity search on vectorized content passed via context or fine-tuning a model on the knowledge graph itself.</p> <p></p> <p>Example sequence diagram of a NL interface to KG solution could look like</p> <p>However, while simplicity and comprehensibility are important, so too are the accuracy and credibility of information. To ensure this, all responses should include links to source data, offering full transparency and traceability.</p> <p>These advancements for LLMs and their integration into knowledge graph interfaces represent an exciting step forward in making complex data more user-friendly and trustworthy.</p>"},{"location":"#2-creating-a-knowledge-graph-from-unstructured-data","title":"2. Creating a Knowledge Graph From Unstructured Data","text":"<p>The second use case showcases the creation of knowledge graphs from a multitude of unstructured data sources, including but not limited to PDFs, HTML pages, and text documents.</p> <p></p> <p>Midjourney has no good concepts of sieves or funnels transforming information :)</p> <p>LLMs interpret various types and meanings in the text, making sense of unstructured data by identifying its inherent structure based on the training data.</p> <p>They can</p> <ul> <li>decipher entities,</li> <li>discern relationships, and</li> <li>eliminate redundancies by recognizing duplicates.</li> </ul> <p>In effect, LLMs can transform a seemingly indistinguishable mass of unstructured text into a well-organized, meaningful knowledge graph of entities and their relationships.</p> <p></p> <p>Example sequence diagram of KG creation could look like</p> <p>Interestingly you can guide LLMs with the appropriate prompts to output structured data directly, e.g. as JSON data structures for node- and relationship-lists, that we can feed directly into the graph database.</p> <p>By leveraging LLMs in this way, we can streamline the knowledge graph creation process, improving efficiency and accuracy. Especially the disambiguation helps with the many variants we humans put into our texts for the same entities and relationships just to entertain the reader.</p> <p>As a result, valuable data becomes easier to access, understand, and use for decision-making.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>As the project progresses, our goal will be to develop prototypes for these use cases. We aim to improve the interaction between you, the users, and your connected data using Neo4j and LLMs.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository: https://github.com/manasa-sk/NaLLM</p>"},{"location":"Fine-Tuning-vs-RAG/","title":"Fine Tuning vs RAG","text":""},{"location":"Fine-Tuning-vs-RAG/#knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation","title":"Knowledge Graphs &amp; LLMs: Fine-Tuning Vs. Retrieval-Augmented Generation","text":"<p>What are the limitations of LLMs, and how to overcome them</p> <p></p> <p>Midjourney\u2019s idea of a knowledge graph chatbot.</p> <p>This is the second blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can find the first and third blog posts here:</p> <ul> <li>Harnessing LLMs With Neo4j</li> <li>Multi-Hop Question Answering</li> </ul> <p>The first wave of hype for Large Language Models (LLMs) came from ChatGPT and similar web-based chatbots, where the models are so good at understanding and generating text that it shocked people, myself included.</p> <p>Many of us logged in and tested its ability to write haikus, motivational letters, or email responses. What became quickly apparent is that LLMs are not only good at generating creative context but also at solving typical natural language processing and other tasks.</p> <p>Shortly after the LLM hype started, people started considering integrating it into their applications. Unfortunately, if you simply develop a wrapper around an LLM API, there is a high chance your application will not be successful as it doesn\u2019t provide additional value.</p> <p>One major problem of LLMs is the so-called knowledge cutoff. The knowledge cutoff term indicates that LLMs are unaware of any events that happened after their training. For example, if you ask ChatGPT about an event in 2023, you will get the following response.</p> <p></p> <p>ChatGPT\u2019s knowledge cutoff date. Image by author.</p> <p>The same problem will occur if you ask an LLM about any event not present in its training dataset. While the knowledge cutoff date is relevant for any publicly available information, the LLM doesn\u2019t have any knowledge about private or confidential information that might be available even before the knowledge cutoff date.</p> <p>For example, most companies have some confidential information that they don\u2019t share publicly but might be interested in having a custom LLM that could answer those questions. On the other hand, a lot of the publicly available information that the LLM is aware of might be already outdated.</p> <p>Therefore, updating and expanding the knowledge of an LLM is highly relevant today.</p> <p>Another problem with LLMs is that they are trained to produce realistic-sounding text, which might not be accurate. Some invalid information is more challenging to spot than others. Especially for missing data, it is very probable that the LLM will make up an answer that sounds convincing but is nonetheless wrong instead of admitting that it lacks the base facts in its training.</p> <p>For example, research or court citations might be easier to verify. A week ago, a lawyer got in trouble for blindly believing the court citations ChatGPT produced.</p> <p>I have also noticed that LLMs will consistently produce assertive, yet false information about any sort of IDs like the WikiData or other identification numbers.</p> <p></p> <p>ChatGPT\u2019s hallucinations. Image by author.</p> <p>Since the response by ChatGPT is assertive, you might expect it to be accurate. However, the given WikiData id points to a farm in England. Therefore, you have to be very careful not to blindly believe everything that LLMs produce. Verifying answers or producing more accurate results from LLMs is another big problem that needs to be solved.</p> <p>Of course, LLMs have other problems, like bias, prompt injection, and others. However, we will not talk about them here. Instead, in this blog post, we will present and focus on the concepts of fine-tuning and retrieval-augmented LLMs and evaluate their pros and cons.</p>"},{"location":"Fine-Tuning-vs-RAG/#supervised-fine-tuning-of-an-llm","title":"Supervised Fine-Tuning of an LLM","text":"<p>Explaining how LLMs are trained is beyond the scope of this blog post. Instead, you can watch this incredible video by Andrej Karpathy to catch up on LLMs and learn about the different phases of LLM training.</p> <p>By fine-tuning an LLM, we refer to the supervised training phase, during which you provide additional question-answer pairs to optimize the performance of the Large Language Model (LLM).</p> <p>Additionally, we have identified two different use cases for fine-tuning an LLM.</p> <p>One use case is fine-tuning a model to update and expand its internal knowledge. \\ In contrast, the other use case is focused on fine-tuning a model for a specific task like text summarization or translating natural language to database queries.</p> <p>First, we will talk about the first use case, where we use fine-tuning techniques to update and expand the internal knowledge of an LLM.</p> <p></p> <p>Supervised fine-tuning flow. Image by author. Icons from Flaticons.</p> <p>Usually, you want to avoid pre-training an LLM as the cost can be upwards of hundreds of thousands and even millions of dollars. A base LLM is pre-trained using a gigantic corpus of text corpus, frequently in the billions or even trillions of tokens.</p> <p>While the number of parameters of an LLM is vital, it is not the only parameter you should consider when selecting a base LLM. Besides the license, you should also consider the bias and toxicity of the pre-training dataset and the base LLM.</p> <p>After you have selected the base LLM, you can start the next step of fine-tuning it. The fine-tuning step is relatively cheap regarding computation cost due to available techniques like the LoRa and QLoRA.</p> <p>However, constructing a training dataset is more complex and can get expensive. If you can not afford a dedicated team of annotators, it seems that the trend is to use an LLM to construct a training dataset to fine-tune your desired LLM (this is really meta).</p> <p>For example, Stanford\u2019s Alpaca training dataset was created using OpenAI\u2019s LLMs. The cost to produce 52 thousand training instructions was about 500 dollars, which is relatively cheap.</p> <p>On the other hand, the Vicuna model was fine-tuned by using the ChatGPT conversations users posted on ShareGPT.com.</p> <p>There is also a relatively fresh project by H2O called WizardLM, which is designed to turn documents into question-answer pairs that can be used to fine-tune an LLM.</p> <p>We haven\u2019t found any recent articles describing how to use a knowledge graph to prepare good question-answer pairs that can be used to fine-tune an LLM.</p> <p>This is an area that we plan to explore during the NaLLM project. We have some ideas for utilizing LLMs to construct question-answer pairs from a knowledge graph context.</p> <p>However, there are a lot of unknowns at the moment. For example, can you provide two different answers to the same question, and the LLM then somehow combines them in its internal knowledge store?</p> <p>Another consideration is that some information in a knowledge graph is not relevant without considering its relationships. Therefore, do we have to pre-define relevant queries, or is there a more generic way to go about it? Or can we use the node-relationship-node patterns representing subject-predicate-object expressions to generate relevant pairs?</p> <p>These are some of the questions we aim to answer in upcoming blog posts.</p> <p>Imagine that you somehow managed to produce a training dataset containing question-answer pairs based on the information stored in your knowledge graph. As a result, the LLM now includes updated knowledge.</p> <p>However, fine-tuning the model didn\u2019t solve the knowledge cutoffs problem since it only pushed the knowledge cutoff to a later date.</p> <p>Therefore, we recommend updating the internal knowledge of an LLM through fine-tuning techniques _only for slowly changing _or updating data. For example, you could use a fine-tuned model to provide tourist information.</p> <p>However, you would run into troubles the second you would want to include special time-dependent (real-time) or personalized promotions in the responses. Similarly, fine-tuned models are not ideal for analytical workflows where you would ask how many new customers the company gained over the last week.</p> <p>At the moment, fine-tuning approaches can help mitigate hallucinations but cannot completely eliminate them. One problem is that the LLMs do not cite their sources when providing answers. Therefore, you have no idea if the answer came from pre-training data, fine-tuning dataset, or was made up by the LLM. Additionally, there might be another possible falsehood source if you use an LLM to create the fine-tuning dataset.</p> <p>Lastly, a fine-tuned model cannot automatically provide different responses depending on the user making the questions. Likewise, there is no concept of access restrictions, meaning that anybody interacting with the LLM has access to all of its information.</p>"},{"location":"Fine-Tuning-vs-RAG/#retrieval-augmented-generation","title":"Retrieval-Augmented Generation","text":"<p>Large language models perform remarkably well in natural language applications like</p> <ul> <li>Text summarization,</li> <li>Extracting relevant information,</li> <li>Disambiguation of entities</li> <li>Translating from one language to another, or even</li> <li>Converting natural language into database queries or scripting code.</li> </ul> <p>Moreover, previously NLP models were most often domain and task-specific, meaning that you would most likely need to train a custom natural language model depending on your use case and domain. However, thanks to the generalization capabilities of LLMs, a single model can be applied to solve various collections of tasks.</p> <p>We have observed quite a strong trend in using retrieval-augmented LLMs, where instead of using LLMs to access its internal knowledge, you use the LLM as a natural language interface to your company\u2019s or private information.</p> <p></p> <p>Retrieval-augmented generation. Image by author. Icons from Flaticons.</p> <p>The retrieval augmented approach uses the LLM to generate an answer based on the additionally provided relevant documents from your data source.</p> <p>Therefore, you don\u2019t rely on internal knowledge of the LLM to produce answers. Instead, the LLM is used only for extracting relevant information from documents you passed in and summarizing it.</p> <p>For example, the ChatGPT plugins can be thought of as a retrieval-augmented approach to LLM applications. The ChatGPT interface with a browsing plugin enabled allows the LLM to search the internet to access up-to-date information and use it to construct the final answer.</p> <p></p> <p>ChatGPT with browsing plugin. Image by author.</p> <p>In this example, ChatGPT was able to answer who won the Oscar for various categories in 2023. But, remember, the cutoff knowledge date for ChatGPT is 2021, so it couldn\u2019t know who won the 2023 Oscars from its internal knowledge. Therefore, it accessed external information through the browsing plugin, which allowed it to answer the question with up-to-date information. Those plugins present an integrated augmentation mechanism inside the OpenAI platform.</p> <p>If you have been watching the LLM space, you might have heard of the LangChain library.</p> <p>The LangChain library can be used to allow LLMs to access real-time information from various sources like Google Search, vector databases, or knowledge graphs. For example, LangChain has added a Cypher Search chain, which converts natural language questions into a Cypher statement, uses it to retrieve information from the Neo4j database, and constructs a final answer based on the provided information.</p> <p>With the Cypher Search chain, an LLM is not only used to construct a final answer but also to translate a natural language question into a Cypher query.</p> <p></p> <p>Cypher search in LangChain. Image by author.</p> <p>Another popular library for retrieval-augmented LLM workflows is LlamaIndex (GPT Index). LlamaIndex is a comprehensive data framework aimed at enhancing the performance of Large Language Models (LLMs) by enabling them to leverage private or custom data.</p> <p>Firstly, LlamaIndex offers data connectors that facilitate the ingestion of a variety of data sources and formats, encompassing everything from APIs, PDFs, and documents to SQL or graph data.</p> <p>This feature allows for an effortless integration of existing data into the LLM. Secondly, it provides efficient mechanisms to structure the ingested data using indices and graphs, ensuring the data is suitably arranged for use with LLMs. In addition, it includes an advanced retrieval and query interface, which enables users to input an LLM prompt and receive back a context-retrieved, knowledge-augmented output.</p> <p>The idea behind retrieval-augmented LLM applications like ChatGPT Plugins and LangChain is to avoid relying on internal LLM knowledge only to generate answers. Instead, LLMs are used to solve tasks like constructing database queries from natural language and constructing answers based on externally provided information or by utilizing plugins/agents for retrieval.</p> <p>The retrieval-augmented approach has some clear advantages over the fine-tuning approach:</p> <ul> <li>The answer can cite its sources of information, which allows you to validate the information and potentially change or update the underlying information based on requirements</li> <li>Hallucinations are more unlikely to occur as you don\u2019t rely on the internal knowledge of an LLM to answer the question and only use information that is provided in the relevant documents</li> <li>Changing, updating, and maintaining the underlying information the LLM uses is easier as you transform the problem from LLM maintenance to a database maintenance, querying and context construction problem</li> <li>Answers can be personalized based on the user context, or their access permission</li> </ul> <p>On the other hand, you should consider the following limitations when using the retrieval-augmented approach:</p> <ul> <li>The answers are only as good as the smart search tool</li> <li>The application needs access to your specific knowledge base, either that be a database or other data stores</li> <li>Completely disregarding the internal knowledge of the language model limits the number of questions that can be answered</li> <li>Sometimes LLMs fail to follow instructions, so there is a risk that the context might be ignored or hallucinations occur if no relevant answer data is found in the context.</li> </ul>"},{"location":"Fine-Tuning-vs-RAG/#summary","title":"Summary","text":"<p>This blog post delves into the limitations of Large Language Models (LLMs), such as</p> <ul> <li>Knowledge cutoff,</li> <li>Hallucinations, and</li> <li>The lack of user customization.</li> </ul> <p>To overcome these, we explored two concepts, namely, fine-tuning and retrieval-augmented use of LLMs.</p> <p>Fine-tuning an LLM involves the supervised training phase, where question-answer pairs are provided to optimize the performance of the LLM. This can be used to update and expand the LLM\u2019s internal knowledge or fine-tune it for a specific task. However, fine-tuning fails to solve the knowledge cutoff issue as it simply pushes the cutoff to a later date. It also cannot fully eliminate hallucinations. Therefore, we recommend using the fine-tuning approach for slowly changing datasets where some hallucinations are allowed. Since fine-tuning LLMs is relatively new, we are eager to learn more about fine-tuning approaches and best practices.</p> <p>The second approach to overcome the limitations of LLMs is the so-called retrieval-augmented generation, where the LLM serves as a natural language interface to access external information, thereby not relying only on its internal knowledge to produce answers. Advantages of the retrieval-augmented approach include source-citing, negligible hallucinations, ease of changing and updating information, and personalization. \\ However, it relies heavily on the intelligent search tool to retrieve relevant information and requires access to the user\u2019s knowledge base. Furthermore, it can only answer queries provided it has the information required to address the question.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository.</p>"},{"location":"Knowledge-Graphs-From-Unstructured-Text/","title":"Knowledge Graphs From Unstructured Text","text":""},{"location":"Knowledge-Graphs-From-Unstructured-Text/#construct-knowledge-graphs-from-unstructured-text","title":"Construct Knowledge Graphs From Unstructured Text","text":"<p>This is the fifth blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we have constructed and publicly displayed demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in a blog post. You can read the previous four blog posts here:</p> <ul> <li>Harnessing LLMs with Neo4j</li> <li>Fine-tuning vs. Retrieval-augmented generation</li> <li>Multi-hop Question Answering</li> <li>Knowledge Graphs &amp; LLMs: Real-Time Graph Analytics</li> </ul> <p>This blog post will explore a use case we investigated during our project: extracting information from unstructured data. Organizations have long faced challenges in extracting meaningful insights from unstructured data. Such data encompasses textual content, images, audio, and other non-tabular formats, holding immense potential yet often remaining difficult to use due to its inherent complexity. Our primary focus in this post will be to extract information from unstructured text by converting it into nodes and relationships.</p> <p></p> <p>Illustration from Imagine.art of \u201cextracting information from unstructured data\u201d</p> <p>Recent years have witnessed significant advancements in natural language processing techniques, revolutionizing the transformation of unstructured data into valuable knowledge. With the emergence of powerful language models like OpenAI\u2019s GPT models and leveraging the power of machine learning, the process of converting unstructured text data into structured representations has become more accessible and efficient.</p> <p>One such representation is knowledge graphs, which offer a robust framework for representing complex relationships and connections among various entities. They provide a structured representation of the data, enabling intuitive querying and exploration of the information contained within. This structured nature allows for advanced semantic analysis, reasoning, and inference, facilitating more accurate and comprehensive decision-making processes.</p> <p></p> <p>Example of Knowledge Extraction Pipeline</p> <p>We will explore how Large Language Models (LLMs) have simplified the conversion of unstructured data into knowledge graphs, using an approach that utilizes the language skills of LLMs to perform nearly all parts of the process. The process can be divided into three steps:</p> <ol> <li>Extracting nodes and edges</li> <li>Entity disambiguation</li> <li>Importing into Neo4j</li> </ol> <p>Let\u2019s walk through each of these steps:</p> <p>1. Extracting nodes and relationships: To tackle this problem, we take the simplest possible approach, where we pass the input data to the LLM and let it decide which nodes and relationships to extract. We ask the LLM to return the extracted entities in a specific format, including a name, a type, and properties. This allows us to extract nodes and edges from the input text.</p> <p>However, LLMs have a limitation known as the context window (between 4 and 16,000 tokens for most LLMs), which can be easily overwhelmed by larger inputs, hindering the processing of such data. To overcome this limitation, we employ a strategy of dividing the input text into smaller, more manageable chunks that fit within the context window.</p> <p>Determining the optimal splitting points for the text is a challenge of its own. To keep things simple, we have chosen to divide the text into chunks of maximum size, maximizing the utilization of the context window per chunk. Additionally, we introduce some overlap from the previous chunk to account for cases where a sentence or description spans across multiple chunks. This approach allows us to extract nodes and edges from each chunk, representing the information contained within it.</p> <p>To maintain consistency in the labeling of different types of entities across chunks, we provide the LLM with a list of node types that were extracted in the previous chunks. Those start forming the extracted \u201cschema.\u201d We have observed that this approach enhances the uniformity of the final labels. For example, instead of the LLM generating separate types for \u201cCompany\u201d and \u201cGaming Company,\u201d it consolidates all types of companies under a \u201cCompany\u201d label.</p> <p>One notable hurdle in our approach is the problem of duplicate entities. Since each chunk is processed semi-independently, information about the same entity found in different chunks will create duplicates when we combine the results. Naturally, this issue brings us to our next step.</p> <p>2. Entity disambiguation: We now have a set of entities. To address the issue of duplication, we employ LLMs once again. First, we organize the entities into sets based on their type. Subsequently, we provide each set to the LLM, enabling it to merge duplicate entities while simultaneously consolidating their properties. We use LLMs for this since we don\u2019t know what name each entity has been given. For example, the initial extraction could have ended up with two nodes: (Alice {name: \u201cAlice Henderson\u201d}) and (Alice Henderson {age: 25}). These reference the same entity and should be merged to a single node with both the name and age property. We use LLMs to accomplish this since it\u2019s great at quickly understanding which nodes actually reference the same entity.</p> <p>By iteratively performing this procedure for all entity groups, we obtain a structured data set that is ready for further processing.</p> <p>3. Importing the data into Neo4j: In the final step of the process, we focus on importing the results we got from the LLM into a Neo4j database. This requires a format that Neo4j can understand. To accomplish this, we parse the generated text from the LLM and transform it into separate CSV files, corresponding to the various node and relationship types. These CSV files are subsequently mapped to a format compatible with the Neo4j Data Importer tool. Through this conversion, we gain the advantage of previewing the data before initiating the import process into a Neo4j database, harnessing the capabilities offered by the Neo4j Importer tool.</p> <p></p> <p>Overview of the application</p> <p>Putting this all together, we have created an application consisting of three parts: a UI to input a file, a controller that executes the previously explained process, and an LLM that the controller talks to. This demo application can be found here, and the source code can be found on GitHub.</p> <p>We also created a version of this pipeline that works essentially in the same way but with the option to include a schema. This schema works like a filter where the user can restrict which types of nodes and relationships and which properties the LLM should include in its result.</p>"},{"location":"Knowledge-Graphs-From-Unstructured-Text/#nallm-graph-construction-demo","title":"NaLLM Graph Construction Demo","text":"<p>nallm-experiments.ew.r.appspot.com</p> <p>If you are interested in learning more about generative AI and knowledge graphs, I would suggest taking a look at Neo4j\u2019s page about generative AI.</p>"},{"location":"Knowledge-Graphs-From-Unstructured-Text/#demonstration","title":"Demonstration","text":"<p>I tested the application by giving it the Wikipedia page for the James Bond franchise and inspected the generated knowledge graph.</p> <p></p> <p>Example of the resulting graph</p> <p>The provided graph subset showcases the generated graph, which, in my opinion, provides a reasonably accurate depiction of the Wikipedia article. The graph primarily consists of nodes representing books and individuals associated with those books, such as authors and publishers.</p> <p>However, there are a few issues with the graph. For instance, Ian Fleming is labeled as a publisher rather than an author for most of the books he wrote. This discrepancy may be attributed to the difficulty the language model had in comprehending that particular aspect of the Wikipedia article.</p> <p>Another problem is the inclusion of relationships between book nodes and the directors of films with the same titles, instead of creating separate nodes for the movies.</p> <p>Finally, It\u2019s worth noting that the LLM appears to be quite literal in its interpretation of relationships, as evidenced by using the relationship type \u201cused\u201d to connect the James Bond character with the cars he drove. This literal approach may stem from the article\u2019s usage of the verb \u201cused\u201d rather than \u201cdrove.\u201d</p> <p>A full video of the demonstration can be found here:</p> <p>Demo of KG Construction</p>"},{"location":"Knowledge-Graphs-From-Unstructured-Text/#problems","title":"Problems","text":"<p>For a demonstration, this approach worked fairly well, and we think it shows that it\u2019s possible to use LLMs to create knowledge graphs. However, we acknowledge certain issues need to be addressed within this approach:</p> <ul> <li>Unpredictable output: This is inherent to the nature of LLMs. We do not know how an LLM will format its results. Even if we ask it to output in a specific format, it might not obey. This might cause problems when trying to parse what it generates. We saw one instance of this while chunking the data: Most of the time, the LLM generated a simple list of nodes and edges, but sometimes the LLM would number the list. Tools to work around this are starting to be released, such as Guardrails and OpenAIs Function API. It\u2019s still early in the world of LLM tooling, so we anticipate that this will not be a problem for long.</li> <li>Speed: This approach is slow and often takes several minutes for just a single reasonably large web page. There might be a fundamentally different approach that can make the extraction go faster.</li> <li>Lack of accountability: There is no way of knowing why the LLM decided to extract some information from the source documents or if the information even exists in the source. The data quality of the resulting knowledge graph is, therefore, much lower than the graph created by processes not leveraging LLMs.</li> </ul>"},{"location":"Knowledge-Graphs-From-Unstructured-Text/#summary","title":"Summary","text":"<p>This blog post explored a use case of Large Language Models with Neo4j to extract insights from unstructured data by converting it into a structured representation in the form of a knowledge graph.</p> <p>We discussed a three-step approach focusing on extracting nodes and relationships, entity disambiguation, and importing the data into Neo4j. By utilizing LLMs, anyone can automate the extraction process and efficiently process large amounts of unstructured data.</p> <p>However, there are challenges to address, including unpredictable output formatting, speed limitations, and the lack of accountability. Despite these issues, the combined power of LLMs and Neo4j offers a promising solution for unlocking the hidden value in unstructured data, even for non-technical users.</p>"},{"location":"Multi-Hop-Question-Answering/","title":"Multi Hop Question Answering","text":""},{"location":"Multi-Hop-Question-Answering/#knowledge-graphs-llms-multi-hop-question-answering","title":"Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering","text":""},{"location":"Multi-Hop-Question-Answering/#retrieve-information-that-spans-across-multiple-documents","title":"Retrieve information that spans across multiple documents","text":"<p>This is the third blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in blog posts. You can see the previous two blog posts here:</p> <ul> <li>Harnessing LLMs With Neo4j</li> <li>Fine-Tuning vs Retrieval-Augmented Generation</li> </ul> <p></p> <p>Midjourney\u2019s idea of an investigative board.</p> <p>In the previous blog post, we learned about the retrieval-augmented approach to overcome the limitations of Large Language Models (LLMs), such as hallucinations and limited knowledge. The idea behind the retrieval-augmented approach is to reference external data at question time and feed it to an LLM to enhance its ability to generate accurate and relevant answers.</p> <p></p> <p>Retrieval-augmented approach to LLM applications. Image by author.</p> <p>When a user asks a question, an intelligent search tool looks for relevant information in the provided Knowledge bases. For example, you might have encountered instances of searching for relevant information within PDFs or a company\u2019s documentation. Most of those examples use vector similarity search to identify which chunks of text might contain relevant data to answer the user\u2019s question accurately. The implementation is relatively straightforward.</p> <p></p> <p>RAG applications using vector similarity search. Image by author.</p> <p>The PDFs or the documentation are first split into multiple chunks of text. Some different strategies include how large the text chunks should be and if there should be any overlap between them. In the next step, vector representations of text chunks are generated by using any of the available text embedding models. That is all the preprocessing needed to perform a vector similarity search at query time. The only step left is to encode the user input as a vector at query time and use cosine or any other similarity to compare the distance between the user input and the embedded text chunks. Most frequently, you will see that the top three most similar documents are returned to provide the context to the LLM to enhance its capability to generate accurate answers. This approach works fairly well when the vector search can produce relevant chunks of text.</p> <p>However, simple vector similarity search might not be sufficient when the LLM needs information from multiple documents or even just multiple chunks to generate an answer.</p> <p>For example, consider the following question:</p> <p>Did any of the former OpenAI employees start their own company?</p> <p>If you think about it, this question can be broken down into two questions.</p> <ul> <li>Who are the former employees of OpenAI?</li> <li>Did any of them start their own company?</li> </ul> <p></p> <p>Information spanning across multiple documents. Image by author.</p> <p>Answering these types of questions is a multi-hop question-answering task, where a single question can be broken down into multiple sub-questions and can require numerous documents to be provided to the LLM to generate an accurate answer.</p> <p>The above-mentioned workflow of simply chunking and embeddings documents in a database and then using plain vector similarity search might struggle with multi-hop questions due to:</p> <ul> <li>Repeated information in top N documents: The provided documents are not guaranteed to contain complementary and complete information needed to answer a question. For example, the top three similar documents might all mention that Shariq worked at OpenAI and possibly founded a company while completely ignoring all the other former employees that became founders</li> <li>Missing reference information: Depending on the chunk sizes, you might lose the reference to the entities in the documents. This can be partially solved by chunk overlaps. However, there are also examples where the references point to another document, so some sort of co-reference resolution or other preprocessing would be needed.</li> <li>Hard to define ideal N number of retrieved documents: Some questions require more documents to be provided to an LLM to accurately answer the question, while in other situations, a large number of provided documents would only increase the noise (and cost).</li> </ul> <p></p> <p>An example where the similarity search might return some duplicated information, while other relevant information could be ignored due to a low K number of retrieved information or embedding distance. Image by the author.</p> <p>Therefore, a plain vector similarity search might struggle with multi-hop questions. However, we can employ multiple strategies to attempt to answer multi-hop questions requiring information from various documents.</p>"},{"location":"Multi-Hop-Question-Answering/#knowledge-graph-as-condensed-information-storage","title":"Knowledge Graph as Condensed Information Storage","text":"<p>If you are paying close attention to the LLM space, you might have come across the idea of using various techniques to condense information for it to be more easily accessible during query time. For example, you could use an LLM to provide a summary of documents and then embed and store the summaries instead of the actual documents. Using this approach, you could remove a lot of noise, get better results, and worry less about prompt token space.</p> <p>The same approach can be applied to summarize conversation history to avoid running into token limit problems.</p> <p>I haven\u2019t seen any articles about combining and summarizing multiple documents as a single record. The problem is probably that there are too many combinations of documents that we could merge and summarize. Therefore, it is perhaps too costly to process all the combinations of documents at ingestion time. \\ However, a knowledge graph can help here too.</p> <p>The process of extracting structured information in the form of entities and relationships from unstructured text has been around for some time and is better known as the information extraction pipeline. The beauty of combining an information extraction pipeline with knowledge graphs is that you can process each document individually, and the information from different records gets connected when the knowledge graph is constructed or enriched.</p> <p></p> <p>Extracting entities and relationships from text to construct a knowledge graph. Image by author.</p> <p>The knowledge graph used nodes and relationships to represent data. In this example, the first document provided the information that Dario and Daniela used to work at OpenAI, while the second document offered information about their Anthropic startup. Each record was processed individually, yet the knowledge graph representation connects the data and makes it easy to answer questions spanning across multiple documents.</p> <p>Most of the newer approaches using LLMs to answer multi-hop questions we encountered focus on solving the task at query time. However, we believe that many multi-hop question-answering issues can be solved by preprocessing data before ingestion and connecting it in a knowledge graph. The information extraction pipeline can be performed using LLMs or custom text domain models.</p> <p>In order to retrieve information from the knowledge graph at query time, we have to construct an appropriate Cypher statement. Luckily, LLMs are pretty good at translating natural language to Cypher graph-query language.</p> <p></p> <p>Using knowledge graphs as part of retrieval-augmented LLM applications. Image by author.</p> <p>In this example, the smart search uses an LLM to generate an appropriate Cypher statement to retrieve relevant information from a knowledge graph. The relevant information is then passed to another LLM call, which uses the original question and the provided information to generate an answer. In practice, you could use different LLMs for generating Cypher statements and answers or use various prompts on a single LLM.</p>"},{"location":"Multi-Hop-Question-Answering/#combining-graph-and-textual-data","title":"Combining Graph and Textual Data","text":"<p>Sometimes, you might want to combine textual and graph data to find relevant information. For example, consider the following question:</p> <p>What is the latest news about Prosper Robotics founders?</p> <p>In this example, you might want to identify the Prosper Robotics founders using the knowledge graph structure and retrieve the latest articles mentioning them.</p> <p></p> <p>Knowledge graph with explicit links between structured information and unstructured text. Image by author.</p> <p>To answer the question about the latest news about Prosper Robotics founders, you would start from the Prosper Robotics node, traverse to its founders, and then retrieve the latest articles mentioning them.</p> <p>A knowledge graph can be used to represent structured information about entities and their relationships, as well as unstructured text as node properties. Additionally, you could employ natural language techniques like named entity recognition to connect unstructured information to relevant entities in the knowledge graph, as shown with the MENTIONS relationship.</p> <p>We believe that the future of retrieval-augmented generation applications is utilizing both structured and unstructured information to generate accurate answers. Therefore, a knowledge graph is a perfect solution because you can store both structured and unstructured data and connect them with explicit relationships, making information more accessible and easier to find.</p> <p></p> <p>Using Cypher and vector similarity search to retrieve relevant information from a knowledge graph. Image by author.</p> <p>When the knowledge graph contains structured and unstructured data, the smart search tool could utilize Cypher queries or vector similarity search to retrieve relevant information. In some cases, you could also use a combination of the two. For example, you could start with a Cypher query to identify relevant documents and then use vector similarity search to find specific information within those documents.</p>"},{"location":"Multi-Hop-Question-Answering/#using-knowledge-graphs-in-chain-of-thought-flow","title":"Using Knowledge Graphs in Chain-of-Thought Flow","text":"<p>Another very exciting development around LLMs is the so-called chain-of-thought question answering, especially with LLM agents. The idea behind LLM agents is that they can decompose questions into multiple steps, define a plan, and use any of the provided tools. In most cases, the agent tools are APIs or knowledge bases that the agent can access to retrieve additional information. Let\u2019s again consider the following question:</p> <p>What is the latest news about Prosper Robotics founders?</p> <p></p> <p>No explicit links between knowledge graph entities and unstructured text. Image by author.</p> <p>Suppose you don\u2019t have explicit connections between articles and entities they mention. The articles and entities could even be in separate databases. In this case, an LLM agent using chain-of-thought flow would be very helpful. First, the agent would decompose the question into sub-questions.</p> <ul> <li>Who are the founders of Prosper Robotics?</li> <li>What is the latest news about them?</li> </ul> <p>Now, an agent could decide which tool to use. Suppose we provide it with a knowledge graph access that it can use to retrieve structured information. Therefore, an agent could choose to retrieve the information about the founders of Prosper Robotics from a knowledge graph. As we already know, the founder of Prosper Robotics is Shariq Hashme. Now that the first question was answered, the agent could rewrite the second subquestion as:</p> <ul> <li>What is the latest news about Shariq Hashme?</li> </ul> <p>The agent could use any of the available tools to answer the subsequent question. The tools can range from knowledge graphs, document or vector databases, various APIs, and more. Having access to structured information allows LLM applications to perform various analytics workflows where aggregation, filtering, or sorting is required. Consider the following questions:</p> <ul> <li>Which company with a solo founder has the highest valuation?</li> <li>Who founded the most companies?</li> </ul> <p>Plain vector similarity search can struggle with these types of analytical questions since it searches through unstructured text data, making it hard to sort or aggregate data. Therefore, a combination of structured and unstructured data is probably the future of retrieval-augmented LLM applications. Additionally, as we have seen, knowledge graphs are also ideal for representing connected information and, consequently, multi-hop queries.</p> <p>While the chain-of-thought is a fascinating development around LLMs as it shows how an LLM can reason, it is not the most user-friendly as the response latency can be high due to multiple LLM calls. However, we are still very excited to understand more about incorporating knowledge graphs into chain-of-thought flows for various use cases.</p>"},{"location":"Multi-Hop-Question-Answering/#summary","title":"Summary","text":"<p>Retrieval-augmented generation applications often require retrieving information from multiple sources to generate accurate answers. While textual summarization can be challenging, representing information in a graph format can offer several advantages.</p> <p>By processing each document separately and connecting them in a knowledge graph, we can construct a structured representation of the information. This approach allows for easier traversal and navigation through interconnected documents, enabling multi-hop reasoning to answer complex queries. Furthermore, constructing the knowledge graph during the ingestion phase reduces the workload during query time, resulting in improved latency.</p> <p>Another advantage of using a knowledge graph is its ability to store both structured and unstructured information. This flexibility makes a knowledge graphs suitable for a wide range of language model (LLM) applications, as it can handle various data types and relationships between entities. The graph structure provides a visual representation of the knowledge, facilitating transparency and interpretability for both developers and users.</p> <p>Overall, leveraging knowledge graphs in retrieval-augmented generation applications offers benefits such as improved query efficiency, multi-hop reasoning capabilities, and support for structured and unstructured information.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository.</p>"},{"location":"Real-Time-Graph-Analytics/","title":"Real Time Graph Analytics","text":""},{"location":"Real-Time-Graph-Analytics/#knowledge-graphs-llms-real-time-graph-analytics","title":"Knowledge Graphs &amp; LLMs: Real-Time Graph Analytics","text":""},{"location":"Real-Time-Graph-Analytics/#understanding-data-points-through-the-context-of-their-relationships","title":"Understanding data points through the context of their relationships","text":"<p>This is the fourth blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can read the previous three blog posts here:</p> <ul> <li>Harnessing LLMs with Neo4j</li> <li>Fine-tuning vs. Retrieval-augmented generation</li> <li>Multi-hop Question Answering</li> </ul> <p></p> <p>Graph data analyst as imagined by Midjourney.</p> <p>Large Language Models (LLMs) have significantly changed data accessibility to the average person. Less than a year ago, accessing the company\u2019s data required technical skills involving proficiency in numerous dashboarding tools or even diving into the intricacies of a database query language. Yet, with the rise of LLMs like ChatGPT, the wealth of knowledge hidden within private databases or accessible via various APIs is now more readily available than ever with the rise of so-called retrieval-augmented LLM applications.</p> <p></p> <p>Retrieval-augmented generation application. Image by author.</p> <p>The idea behind retrieval-augmented applications is to retrieve additional information from various sources to allow the LLM to generate better and more accurate results. It seems OpenAI has also picked up on this trend as they introduced OpenAI functions recently. The new OpenAI models are trained to use provide parameters to functions (or what other libraries call tools), whose signatures and descriptions are passed in the context, to retrieve additional information at query time if needed.</p> <p>We have observed a strong bias for vector similarity search in retrieval-augmented applications. If you opened Twitter or LinkedIn in the past three months, you might have seen the various \u201cChat with your PDFs\u201d applications. In those examples, the implementation is relatively straightforward. The text is extracted from PDFs, split into chunks if needed, and finally stored in a vector database along with its text embedding representations.</p> <p>The barrier to entry with these types of applications is low, especially if you are dealing with small amounts of data. It is fascinating that so many articles giving the impression that only vector databases are relevant for retrieval-augmented applications are published nowadays.</p> <p>While there is immense power in vector similarity-based information retrieval from unstructured text, we believe that structured information has an important role to play in LLM applications.</p> <p>Last time we wrote about multi-hop question answering and how knowledge graphs can help solve problems of retrieving information from multiple documents to generate an accurate answer. Additionally, we hinted that vector similarity search is not designed for analytics workflows, where we rely on structured information.</p>"},{"location":"Real-Time-Graph-Analytics/#knowledge-graphs-llms-multi-hop-question-answering","title":"Knowledge Graphs &amp; LLMs: Multi-Hop Question Answering","text":""},{"location":"Real-Time-Graph-Analytics/#retrieve-information-that-spans-across-multiple-documents","title":"Retrieve information that spans across multiple documents","text":"<p>medium.com</p> <p>For example, questions like:</p> <ul> <li>Who could introduce me to Emil Eifr\u00e9m (CEO of Neo4j)?</li> <li>How is ALOX5 gene related to Crohn\u2019s disease?</li> <li>When we have a particular microservice outage, how does it affect our products?</li> <li>How does a flight delay propagate through the network?</li> <li>Which users can be credited for a social media post virality?</li> </ul> <p>All these questions require highly-connected information to be able to answer the question accurately. For example, to learn who can introduce you to Emil, you need information about relationships between people.</p> <p>On the other hand, you need to map dependencies between your microservices and products in order to evaluate the scale and severity of a particular microservice failure.</p> <p>In this blog post, we will introduce some of the frequent use cases of real-time graph analytics that you might want to implement into your LLM applications.</p>"},{"location":"Real-Time-Graph-Analytics/#finding-shortest-paths","title":"Finding (Shortest) Paths","text":"<p>Relationships are first-class citizens in native graph databases. Although knowledge graphs allow you to perform typical aggregations and filtering to answer questions like \u201cHow many customers did we get this week?\u201d, we will focus more on analytical use cases where traversing the relationships is the main component. One such example is finding the shortest or all possible paths between data points. For example, to answer the question:</p> <p>Who could introduce me to Emil Eifrem?</p> <p>We would have to find the shortest path between myself and Emil Eifrem in the graph.</p> <p></p> <p>Single shortest path. Image by author.</p> <p>Another use case where finding the shortest paths in real-time would come in handy is in any sort of transportation, logistics, or routing application. In these applications, you might want to evaluate the top N shortest paths to ensure some fallback plan if something unexpected happens.</p> <p></p> <p>Top two shortest paths between stops in Rome. Image by author.</p> <p>This image visualizes the top 2 shortest paths between two stops in Rome. Such shortest paths could be optimized for distance, time, cost, or a combination.</p> <p>Another domain where finding paths between data points in your LLM applications is the biomedical domain. In the biomedical domain, you are dealing with genes, proteins, diseases, drugs, and more. What\u2019s perhaps more important is that these entities do not exist in isolation but have complex, often multilayered relationships with each other.</p> <p>For instance, a gene may be associated with multiple diseases, a protein may interact with numerous other proteins, a disease might be treatable by a variety of drugs, and a drug could have multiple effects on different genes and proteins.</p> <p>Given the staggering amount of biomedical data available, the number of potential relationships between these data points is enormous and is a great candidate to be represented as a knowledge graph.</p> <p></p> <p>All shortest paths between Crohn\u2019s disease and ALOX5 gene using Hetionet dataset. Image by author.</p> <p>Biomedical knowledge graph can support LLM applications where users are be interested in answering questions like</p> <p>How is ALOX5 gene related to Crohn\u2019s disease?</p> <p>While most LLM applications we see today generate answers as a natural language, there is also an excellent opportunity for returning responses in the form of line, bar, or even network visualizations. Often the LLM can even return the configuration structure needed for the charting libraries.</p>"},{"location":"Real-Time-Graph-Analytics/#information-propagating-through-network","title":"Information Propagating Through Network","text":"<p>Another strong knowledge graph fit is domains with networks of dependencies. For example, you could have a knowledge graph containing the complete microservice architecture of your system. Such a knowledge graph would allow you to power a DevOps chatbot that would enable you to evaluate the architecture in real-time and perform what-if analysis.</p> <p></p> <p>Microservices &amp; People graph. Screenshot from https://www.youtube.com/watch?v=_qakAUjXiek&amp;t=2517s</p> <p>Also Rhys Evans presented how the Financial Times manages their infrastructure as a graph.</p> <p>Another domain that comes to mind is the supply chain.</p> <p></p> <p>Incorporating supply chain data into knowledge graphs can significantly enhance the capabilities of large language applications. This approach allows us to structure complex supply chain information into nodes and relationships, thereby generating a holistic picture of how materials, components, and products flow from suppliers to customers. The inherent interconnections and dependencies become evident and analyzable.</p> <p>For language applications, this enables deeper context understanding and knowledge generation. For instance, an AI model like ChatGPT can leverage this data structure to produce more accurate and insightful responses about supply chain scenarios, disruptions, or management strategies. It could comprehend and explain the ripple effects of a shortage of a certain component, predict potential bottlenecks, or suggest optimization strategies.</p> <p>By aligning the intricacies of supply chain dynamics with the cognitive abilities of AI, we can bolster the functionality and value of large language applications in a multitude of industrial and commercial contexts.</p>"},{"location":"Real-Time-Graph-Analytics/#social-network-analysis-and-data-science","title":"Social Network Analysis and Data Science","text":"<p>What if your company chatbot went beyond documentation and helped deliver insights and recommendations as part of the people analytics?</p> <p></p> <p>Knowledge graphs in HCM can serve as an invaluable tool in driving people analytics within a company, primarily by creating a robust, interconnected system of information that allows for a deep, holistic understanding of employee behavior, skills, competencies, interactions, and performance. Essentially, a knowledge graph captures and links complex employee data \u2014 including demographic information, role history, project involvement, performance indicators, and skillsets \u2014 allowing for multi-faceted analysis.</p> <p>This combination of connected data and ML powered tools enable human resources and team leaders to uncover hidden patterns, identify high-potential individuals, predict future performance, assess skill gaps, and inform training needs, thereby fueling data-driven decision-making. By leveraging a knowledge graph, companies can streamline talent management and development processes, enhancing overall organizational effectiveness and fostering a culture of continuous learning and improvement.</p> <p>Incorporating a chatbot interface into this knowledge graph-driven people analytics system could revolutionize the way companies approach HR and talent management. Here\u2019s how:</p>"},{"location":"Real-Time-Graph-Analytics/#user-friendly-access-to-complex-data","title":"User-Friendly Access to Complex Data","text":"<p>A chatbot interface provides an intuitive, conversational manner for users to interact with complex datasets. Employees, managers, or HR staff wouldn\u2019t need to understand intricate databases or analytics tools; they could simply ask the chatbot questions about employee performance, skills, or team dynamics.</p> <p>The chatbot, equipped with natural language processing capabilities, would interpret the question, retrieve the relevant information from the knowledge graph, and deliver the response in an understandable format.</p>"},{"location":"Real-Time-Graph-Analytics/#real-time-insights","title":"Real-Time Insights","text":"<p>The chatbot interface could offer immediate access to data insights, enabling timely decisions.</p> <p>If a manager wanted to know how many projects are in the pipeline and which people are a good fit and available for a specific project, they could ask the chatbot and get an answer in real-time rather than waiting for a comprehensive report.</p>"},{"location":"Real-Time-Graph-Analytics/#scalable-training-and-support","title":"Scalable Training and Support","text":"<p>The chatbot could provide individualized support to employees, answering questions about company policies, procedures, or career development opportunities.</p> <p>It could even deliver personalized training recommendations (and perhaps the actual training itself) based on an individual\u2019s role, skills, and career goals. This would democratize access to learning and development resources, making it easier for employees to upskill or reskill.</p>"},{"location":"Real-Time-Graph-Analytics/#predictive-analysis","title":"Predictive Analysis","text":"<p>Advanced AI chatbots could analyze patterns and trends from the knowledge graph to make predictions, such as which employees might be at risk of leaving the company or what skills may be in demand in the future. These predictive analytics capabilities could help companies be proactive rather than reactive in their HR strategies.</p> <p>In essence, integrating a chatbot interface with a knowledge graph-driven people analytics system would make complex employee data more accessible, actionable, and useful to all members of an organization. It would be a game-changer in talent management and development, driving a more data-informed, proactive, and personalized approach to HR.</p>"},{"location":"Real-Time-Graph-Analytics/#summary","title":"Summary","text":"<p>In conclusion, as we traverse deeper into the era of large language models, we must keep in mind the enormous potential of knowledge graphs in structuring, organizing, and retrieving information in these applications. The combination of structured and unstructured data retrieval paves the way for more accurate, reliable, and impactful results, extending beyond natural language answers into the realm of visually represented information.</p> <p>Despite the popularity of vector similarity-based data retrieval (recall), we should not underestimate the role of structured information and the immense value it brings to LLM applications. Whether it\u2019s finding the shortest paths, understanding complex biomedical relationships, analyzing supply chain scenarios, or revolutionizing HR with people analytics, the applications of knowledge graphs are vast and profound. We believe the future of LLM-based applications is the combination of vector similarity search approach coupled with database query languages such as Cypher.</p> <p>Through this blog post, we\u2019ve explored some exciting real-time graph analytics use cases that could be implemented into your LLM applications. This is only the beginning. We anticipate a future where large language models will work more cohesively with knowledge graphs, bringing about more innovative solutions to real-world problems.</p>"},{"location":"Specs-Good-Intructor/","title":"Specs Good Intructor","text":"<p>Specification, Design and development of a Conversation GenAI powered Dashboard with a recommendation engine for Identification of the Traits of a Good Instructor  and for suggesting improvement approaches based on the traits of the top performers</p>"},{"location":"Specs-Good-Intructor/#background","title":"Background","text":"<ul> <li>Instructors within an educational system, comprises a combination of various educational groups, faculty members, researchers, students, as well as administrative staff. </li> <li>Each group of faculty members contributes to different fields. </li> <li>The management of all data records related to the performance and activities of the faculty and its members leads to better monitoring, identification of weaknesses, strengths and the traits of a \u201cgood\u201d instructor from the top performers allowing the educational institution to characterize instructors which can be used to improve the overall performance of the faculty. </li> <li>A conversationally enabled analytical dashboard is the primary interaction tool that is used for monitoring and metricizing faculty performance and using it to identify the traits of good instructors.</li> </ul>"},{"location":"Specs-Good-Intructor/#objectives-of-the-effort","title":"Objectives of the Effort","text":"<p>The objective of this effort is to develop a functional, conversationally driven dashboard for the faculty of BITS and metricize and identify the traits of a good instructor and use that to suggest improvements in its teaching staff\u2019s efficacy. </p>"},{"location":"Specs-Good-Intructor/#effort-approach-and-phases","title":"Effort Approach and Phases","text":"<p>This effort uses a mixed methods approach that will be conducted in four main phases. </p> <ul> <li>In the first phase, all the resources related to the functional dashboard are reviewed in order to identify its functional and operational requirements. </li> <li>In the second phase, the detailed feature statements and capabilities of the software are determined by both qualitative (interview) and quantitative (Delphi) methods. <ul> <li>In this phase, eight people will be interviewed during the qualitative phase, and thematic analysis will be used to analyse the data. </li> <li>For the quantitative step, the two- round Delphi technique will be conducted by the purposeful selection of 21 individuals. </li> </ul> </li> <li>In the third phase, <ul> <li>Deployment of the Spanda AI Software Platform will be first performed </li> <li>A custom application comprising of a conversationally enabled GenAI powered dashboard along with the associated retrieval augmented language model and the recommendation engine is developed using Python programming language in an IDE</li> <li>The application will be deployed on a private cloud to ensure data privacy </li> <li>15 people among faculty members and managers, who are identified as the users of the dashboard software, are selected to evaluate the software. </li> <li>Users\u2019 satisfaction with the dashboard software is assessed using a Dashboard Assessment Usability Model. </li> <li>Usability Evaluation Criteria for Dashboards<ul> <li>According to the review of other questionnaires used in previous studies, the following criteria are identified for dashboard evaluation: usefulness, operability, learnability, ease of use, suitability for tasks, improvement of situational awareness, satisfaction, user interface, content, and system capabilities</li> <li>Usefulness\u00a0<ul> <li>Usefulness is usually defined as meeting a customer's needs or providing a competitive advantage with the product's attributes or benefits. Designers, generally, aim to deliver useful products. The \u201cusefulness\u201d criterion was used instead of \u201ceffectiveness and efficiency\u201d to evaluate the usability of dashboards.</li> </ul> </li> <li>Operability\u00a0<ul> <li>It refers to a user's ability to use and control a dashboard for performing their tasks. In the present study, operability included criteria, such as representation of data in detail, access to various filters and reports, and ability to correct errors and support user. The user control is measured under the \u201coperability\u201d criterion.</li> </ul> </li> <li>Learnability\u00a0<ul> <li>Learnability is a quality of software interface that allows users to quickly become familiar with them and able to make good use of all their features and capabilities.</li> </ul> </li> <li>Ease of Use\u00a0<ul> <li>It is a fundamental concept explaining how easily users can employ a dashboard. This criterion was used for dashboard evaluation in the EUCS, Health-ITUES, and TAM questionnaires.</li> </ul> </li> <li>Suitability for Tasks\u00a0<ul> <li>This criterion can help to assess if users can find out whether a product or system is appropriate for their needs. It provides support for the users' daily activities and ensures the compatibility and organization of data on the screen with the user's tasks.</li> </ul> </li> <li>Improvement of Situational Awareness\u00a0<ul> <li>Situation awareness at a fundamental level is about understanding what is going on and what might happen next. The criteria for evaluating situational awareness were divided into instability representation, complexity representation, variability representation, arousal support, concentration support, spare mental capacity support, and division of attention.</li> </ul> </li> <li>Satisfaction<ul> <li>This criterion refers to satisfaction with the features, capabilities, and ease of use of a dashboard.</li> </ul> </li> <li>User Interface\u00a0<ul> <li>It consists of visual and interactive tools. Visual tools in a dashboard involve color coding for data visualization, histogram plots, pie charts, bar graphs, gauges, data labels, and geographic maps. The interactive techniques also include customizable searching, summary view, drill up and drill down, data ordering and filtering, zoom in and zoom out, and real-time feature.</li> </ul> </li> <li>Content\u00a0<ul> <li>This criterion involves the quantity and quality of data displayed by a dashboard. The quantity of displayed data was measured using two questionnaires (SART and PSSUQ), while quality was measured using SART. The amount of displayed data and their compatibility with the users' tasks were also evaluated, and data accuracy, timeliness (being up-to-date), comprehensiveness, and relevance were used for measuring data quality.</li> </ul> </li> <li>System Capabilities\u00a0<ul> <li>Evaluation of compatibility is a criterion to assess software in terms of compatibility with work-related requirements. The dashboard capabilities are evaluated to determine how well its compatibility to work-related processes and how well it satisfies the users' data requirements.</li> </ul> </li> </ul> </li> <li>The collected data will be analysed using descriptive statistics and data analysis software to suggest feedback and improvement as well as to assess large-scale deployability</li> </ul> </li> </ul>"},{"location":"Specs-Good-Intructor/#final-product","title":"Final Product","text":"<ul> <li>The Final product of this study is a GenAI powered dashboard for monitoring, evaluating performance, characterizing a \u201cgood\u201d instructor and providing improvement recommendations to supporting the improvement of instructors and resources at the faculty level. </li> <li>The steps of designing this dashboard can be a basis for developing better dashboards for evaluating other faculties or even other universities.</li> </ul>"},{"location":"Specs-Good-Intructor/#definitions-description-of-current-shortcomings-and-needs","title":"Definitions, Description of current shortcomings and needs","text":"<ul> <li>A faculty, as an educational system, consists of a combination of various educational groups, faculty members, researchers, students, and administrative staff. </li> <li>Each faculty member contributes to different areas (teaching, research, and management). </li> <li>A faculty is a place where different types of courses, learning activities, conferences and conventions are held. </li> <li>The data related to these activities with the participation of faculty members are facts and information resulting from academic activities. </li> <li>The data of a faculty refers to the information linked with the academic performance of its instructors and lecturers, such as the details of academic services and contributions, courses completed, student evaluations provided, the number of annual research publications, and the number of committees in which the faculty member is a member of etc.</li> <li>Although the collection, management, and reporting of faculty data is crucial for each faculty member, as well as for the institution itself as a complete establishment, numerous gaps exist in this area. </li> <li>While a faculty member may be involved in several activities, most of these activities are not documented and recognized because universities lack a central system for effectively recording these data and presenting a comprehensive report of such activities and performance feedback that can be used to guide faculty performance improvement.</li> <li>Currently, typically in many universities, different independent systems host faculty data. The lack of internal communication between these systems causes these data to be enclosed in a contained silo. </li> <li>Retrieving data from multiple systems is often a manual and a difficult process for administrative staff and faculty members. </li> <li>As these data are not analysed using Generative AI techniques or merged to provide an integrated picture, their trends and inter-relationships cannot be exploited </li> <li>This is a lost opportunity <ul> <li>to acquire data</li> <li>to discover information</li> <li>extract knowledge about instructors and their methods</li> <li>identify the traits of good instructors from the top performers </li> <li>using those identified traits to recommend performance improvement using AI powered recommender systems) </li> <li>Positively impacting the overall quality of education delivered.</li> </ul> </li> <li>Currently, the data recording aspect is inconsistent and inadequate in most higher education institutions, </li> <li>There also is only partial automation in terms of recording and sharing data between different systems and constituents. </li> <li>Therefore, faculty members and managers spend a lot of time and effort in manual data entry to gather or track the details of academic activities and drive their assessments and performance improvements. </li> <li>Despite the fact that the manual entering of data is unavoidable in some cases, use of intelligent automation, enabling interoperability between systems to prevent the duplicate recording of data and use of AI to enable dashboards is not well implemented today. </li> <li>In addition, faculty members have inadequate time and skills to perform statistical analyses on data (e.g., findings correlations, querying, recommendations etc.) and extrapolate valuable interpretations, targeted feedback, or practical complementary objectives as the dashboards are poorly design and do not provide easy to use conversational interfaces or AI powered engines.</li> <li>As a data management as well as decision support tool, conversationally enabled dashboards are one of the most effective and renowned forms of data objectification. </li> <li>A dashboard can be defined as: \u201ca tool for visualization that provides the possibility for acquiring awareness, finding trends, planning, and real comparisons. </li> <li>These items are repeatedly embodied in a simple and functional user interface. </li> <li>A dashboard of accumulated data effectively presents multiple sources and a comprehensive summary of important information that can be assimilated by faculty members at a glance, queried, directed to identify traits of \u201cgood\u201d instructors based on top performers. </li> <li>These dashboards enable organizations to measure, monitor, characterise and improve the performance of faculty members and drive continuous improvement using various recommendation methods more effectively. </li> <li>These dashboards build on the foundations of business intelligence, data integration infrastructure, data science and Generative AI and are used for monitoring, analysis, and management and decision support.</li> <li>Developing a faculty performance evaluation, benchmarking and improvement dashboard is useful for quickly sharing with faculty members information about their performance in a way that requires minimal effort and helps them better understand the data by querying the data using language-based interfaces (chatbots) and asking for suggestions on improvement (recommendation engines) as well as identifying those traits (Factor Analysis, Causal Analysis) that can be used to identify an effective teacher. </li> <li>Observing, querying and interpreting the data presented in large tables and lengthy reports are exhausting and time-consuming for faculty members. In other words, a dashboard, if designed appropriately with conversational interfaces, can help faculty members quickly spot their strengths and areas of progress and identify the trends and steps necessary for improvement and recommend/suggest methods for improvement.</li> <li>Based on the current state explorations, there are substantial gaps in the reporting and management of faculty data. </li> <li>Therefore, it is necessary to develop a comprehensive dashboard for monitoring and evaluating the performance of instructors across various activities such as education, research, cultural fit, student affairs participation, resource management effectiveness and technology development.</li> </ul>"},{"location":"Specs-Good-Intructor/#approach-details","title":"Approach Details","text":"<ul> <li>The effort will be carried out by the combined method of consecutive mixed designs. </li> <li>In a sequential design, the data collection and data analysis of one component take place after the data collection and data analysis of the other component and depends on the outcomes of the other component. </li> <li>Mixed Methods Research combines both closed-ended response data (quantitative) and open-ended personal data (qualitative).</li> <li>The research environment is the Faculty of BITS. The study and software development must have obtained an ethical approval and will be conducted in four phases.</li> </ul>"},{"location":"Specs-Good-Intructor/#phase-one-identification-of-functional-and-non-functional-requirements-of-performance-dashboards-and-performance-indicators-and-traits-of-good-instructors-through-interviews-and-systematic-research","title":"Phase One: Identification of functional and non-functional requirements of performance dashboards and performance indicators and traits of good instructors through interviews and systematic research","text":"<ul> <li>The aim of this phase is to extract the functional parameters of the faculty, as well as the capabilities of the performance dashboard. </li> <li>In this step, the research and interviews are performed using a combination of methods. The indicators of performance identified are divided into five different groups, including education, research, cultural and student affairs, resource management, and development &amp; technology, each of which has its own performance indicators.</li> </ul>"},{"location":"Specs-Good-Intructor/#phase-two-requirements-of-the-instructor-traits-dashboard-from-the-perspective-of-users","title":"Phase Two: Requirements Of the Instructor Traits Dashboard From The Perspective Of Users","text":"<ul> <li>This phase is conducted in two steps. </li> <li>In the first step, a qualitative study is conducted to identify the requirements of the performance dashboard software.<ul> <li>For this purpose, eight educational group directors and faculty directors are selected by purposeful sampling for interviews. </li> <li>The average duration of each interview will be 30 minutes. </li> <li>At this stage, after coordinating with the interviewee and obtaining informed consent, the voice of the interview is recorded using an electronic recorder, and then its text is transcribed verbatim in Microsoft Word.</li> <li>The questions of the interview are related to the functional and non-functional requirements of the dashboard, as well as the performance preferences of users. </li> <li>After transcription, the interviews are subjected to code extraction and then thematic analysis. <ul> <li>Phase 1: Familiarizing yourself with your data </li> <li>Phase 2: Generating initial codes </li> <li>Phase 3: Searching for themes </li> <li>Phase 4: Reviewing themes</li> <li>Phase 5: Defining and naming </li> <li>Phase 6: Producing the report themes.</li> </ul> </li> </ul> </li> <li>In the second step, a questionnaire is designed to identify the key performance indicators of the faculty using the two- round Delphi technique. <ul> <li>Twenty individuals are purposefully selected among academic members, educational group directors, and faculty directors. </li> <li>In the first step of the Delphi technique, a questionnaire with three-choice questions (disagree, no opinion, and agree) and an open-ended question at the end of each section are completed, so people could state if they think anything should be added to the questionnaire for the second step of the Delphi technique. </li> <li>In the second step of the Delphi Technique, the indicators proposed are added and subjected to a poll. </li> <li>For data analysis, items with higher than 75% agreement are accepted, those with an agreement between 50\u201375% enter the second round of Delphi, and items with &lt;\u200950% agreement are omitted from the questionnaire.</li> </ul> </li> </ul>"},{"location":"Specs-Good-Intructor/#phase-three-software-development-and-deployment","title":"Phase Three: Software Development and Deployment","text":"<ul> <li>For writing the code of this software, the Spanda Platform, Python and Web Technologies are used. </li> <li>The interface of the software is designed using Html, JQuery, CSS, and Javascript languages to be run from standard browsers</li> <li>Database software is used for designing tables and managing the database.</li> <li>Private Cloud connectivity is available</li> </ul>"},{"location":"Specs-Good-Intructor/#phase-four-evaluation-of-user-satisfaction","title":"Phase Four: Evaluation Of User Satisfaction","text":"<ul> <li>In this phase, 15 of the academic members and managers of the faculty who are the users of the dashboard software are chosen. </li> <li>In order to evaluate user satisfaction with the dashboard software, a 20 question Dashboard Assessment Usability Model scored based on a five-point Likert scale (1 = \"completely disagree\"; 5 = \"Completely agree\") will be used. </li> <li>In addition, two open-ended questions are presented to the participants so that they can express their viewpoints and recommendations. </li> <li>This scale will evaluate the dimensions of satisfaction (four questions), effectiveness (two questions), efficiency (two questions), operability ( ve questions), learnability (four questions), user interface aesthetics (one question), appropriate recognizability (one question), and accessibility (one question).</li> </ul>"},{"location":"Specs-Good-Intructor/#open-ended-questions","title":"Open-ended Questions","text":"<ul> <li>Is there any additional information besides the ones provided here that you would want to see in the dashboard?</li> <li>Do you have any other comments or suggestions that you would like to share with us?</li> <li>The validity and reliability of the questionnaire have been confirmed previously. </li> <li>In the final step, the data are presented in tables using descriptive statistics such as frequency and percentage. Data analysis is conducted in Jupyter Notebook software.</li> </ul>"},{"location":"Specs-Good-Intructor/#ethical-considerations","title":"Ethical considerations","text":"<ul> <li>This study has been approved by the University Ethics Committee.</li> <li>The confidentiality and anonymity of participants\u2019 information are strictly observed. </li> <li>During interviews, participants\u2019 voices are recorded after obtaining their written informed consent.</li> <li>Participants\u2019 information will not be disclosed in any publication form, and they will be clearly explained that they have the right to withdraw from the study at any time.</li> </ul>"},{"location":"Specs-Good-Intructor/#brief-value-discussion","title":"Brief Value Discussion","text":"<ul> <li>This effort aims to design, implement and evaluate the effectiveness of a GenAI powered performance dashboard for functional monitoring, evaluation, and identification of traits of a good instructor to help improve instructor performance and resource management at the faculty level. </li> <li>The steps used for developing this dashboard can provide a basis for designing better performance trait identification and recommendation engines for improvement of instructor performance based on the trairs of good instructors gleaned by analysis of top instructors.</li> <li>Regarding the importance of information integration in organizations such as universities, it is essential to trace the flow and dimensions of information. </li> <li>The lack of proper management of information resources can impede the efficient identification of the traits of good teachers from the top performers and thus hinder achieving organizational goals </li> <li>Motivating instructor improvement by working with integrated information presented via graphical dashboards and using conversation interfaces, language models and recommendation engines is possible</li> <li>Eliminating redundant work in different departments, retrieval of similar information, and finally, duplicate flowing of this information into multiple organizational databases, which requires spending extra time and costs to reuse them is possible through this work. </li> <li>The establishment and use of comprehensive information resources play a strategic role in the qualitative development of universities instructional staff and their transformation into pioneer organizations and contribute a substantial role in achieving the strategic goals of the university. </li> <li>The information obtained from the information system provides a powerful management tool in the higher education system. </li> <li>Because of providing timely and accurate information, AI powered conversation dashboards with integrated recommendation systems are considered the most powerful systems to fulfill the informational needs of organizations, including universities, and to handle bulk amounts of organizational data about insttructiors.</li> <li>Performance Improvement can be achieved based on the identification of the traits of the top faculty performers through a performance dashboard, and using a recommendation engine in combination with a performance coaching process through which the function of instructors is formally and regularly assessed at certain intervals to ensure continuous improvement. </li> <li>Evaluation of the performance of academic members refers to the regular assessment of their educational/research activities and determining to what extent the goals of the educational system, according to predetermined criteria, can be achieved. </li> <li>Functional monitoring refers to the real-time observation of the faculty\u2019s key performance indicators. </li> <li>Faculty resource management encompasses being informed of the current situation of human resources and equipment and figuring out optimal ways of deploying them to improve overall faculty performance.</li> <li>Despite the strengths of this approach, we may face some challenges while conducting various phases of this research. </li> <li>For example, in phase one, participants may refuse full cooperation in completing the questionnaire or conducting the interview due to their busy work schedules. </li> <li>We will try to distribute a considerable number of questionnaires among users to obviate this challenge. </li> <li>During the implementation phase, the software designed may not be suitably integrated with other organizational systems, interfering with information exchange. </li> <li>This challenge will be addressed by writing data processing pipelines that normalize and integrate data from various sources.</li> </ul>"}]}