{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#knowledge-graphs-llms-harnessing-large-language-models-with-neo4j","title":"Knowledge Graphs &amp; LLMs: Harnessing Large Language Models with Neo4j","text":"<p>This is the first blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we have been writing about our findings in blog posts. You can read the later blog posts here:</p> <ul> <li>Fine-Tuning vs Retrieval-Augmented Generation</li> <li>Multi-Hop Question Answering</li> </ul> <p>Large Language Models (LLMs) like ChatGPT have taken the world by storm in 2023 due to their ability to understand and generate human-like text. Their capacity to adapt to different conversational contexts, answer questions across a wide range of topics, and even simulate creative writing has revolutionized the way humans and machines interact, sparking a new wave of artificial intelligence applications.</p> <p></p> <p>Steampunk computer wall with a magic mirror operated by ants running in transparent tubes (Midjourney)</p> <p>Thanks to their ability to \u201cunderstand\u201d, generate, and refine human-like text, LLMs offer us new methods for working with data. Our team at Neo4j has started a project to explore, develop, _and showcase_ practical uses of these LLMs in conjunction with Neo4j.</p> <p>One key aspect of this project is the integration of graph database technology and concepts into the LLM application stack. By doing so, we expect to enhance the accuracy, transparency, _and predictability_ of the model output and open up new use-cases both for using LLMs as well as databases.</p> <p>As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute.</p> <p>_While we base our project on our current understanding and technology, we fully acknowledge that this is a rapidly advancing field, and future findings may refine our approach. Consequently, our perspective and strategies are subject to change in response to new data and technological progress.</p>"},{"location":"#identifying-the-real-world-use-cases","title":"Identifying the Real-World Use Cases","text":"<p>The initiation phase of our project focused on the identification of real-world use cases, which would form the basis for our upcoming solutions. After thorough research, market analysis, and customer interactions, we\u2019ve narrowed down two initial use cases that frequently feature in our conversations with customers.</p>"},{"location":"#1-natural-language-interface-to-a-knowledge-graph","title":"1. Natural Language Interface to a Knowledge Graph","text":"<p>Our first use case focuses on developing a natural language interface for knowledge graphs. The goal is to create a user interface that simplifies the process of data selection, querying and processing, making data more accessible and easier to understand.</p> <p>Allowing you to \u201ctalk to your database\u201d.</p> <p></p> <p>Midjourney imagination of a natural language interface for a KG</p> <p>The preferred method for this is a chat-like interface that would generate database queries based on the user question and the inferred schema of the database.</p> <p>Based on feedback from our users, there is a significant demand for natural language responses over just citing data and linking to sources. By utilizing LLMs, we can provide these responses, presenting information in a way that mimics natural, human conversation.</p> <p>We\u2019re exploring techniques to inform the LLMs about the content of the knowledge graph. This could involve a similarity search on vectorized content passed via context or fine-tuning a model on the knowledge graph itself.</p> <p></p> <p>Example sequence diagram of a NL interface to KG solution could look like</p> <p>However, while simplicity and comprehensibility are important, so too are the accuracy and credibility of information. To ensure this, all responses should include links to source data, offering full transparency and traceability.</p> <p>These advancements for LLMs and their integration into knowledge graph interfaces represent an exciting step forward in making complex data more user-friendly and trustworthy.</p>"},{"location":"#2-creating-a-knowledge-graph-from-unstructured-data","title":"2. Creating a Knowledge Graph From Unstructured Data","text":"<p>The second use case showcases the creation of knowledge graphs from a multitude of unstructured data sources, including but not limited to PDFs, HTML pages, and text documents.</p> <p></p> <p>Midjourney has no good concepts of sieves or funnels transforming information :)</p> <p>LLMs interpret various types and meanings in the text, making sense of unstructured data by identifying its inherent structure based on the training data.</p> <p>They can</p> <ul> <li>decipher entities,</li> <li>discern relationships, and</li> <li>eliminate redundancies by recognizing duplicates.</li> </ul> <p>In effect, LLMs can transform a seemingly indistinguishable mass of unstructured text into a well-organized, meaningful knowledge graph of entities and their relationships.</p> <p></p> <p>Example sequence diagram of KG creation could look like</p> <p>Interestingly you can guide LLMs with the appropriate prompts to output structured data directly, e.g. as JSON data structures for node- and relationship-lists, that we can feed directly into the graph database.</p> <p>By leveraging LLMs in this way, we can streamline the knowledge graph creation process, improving efficiency and accuracy. Especially the disambiguation helps with the many variants we humans put into our texts for the same entities and relationships just to entertain the reader.</p> <p>As a result, valuable data becomes easier to access, understand, and use for decision-making.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<p>As the project progresses, our goal will be to develop prototypes for these use cases. We aim to improve the interaction between you, the users, and your connected data using Neo4j and LLMs.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository: https://github.com/manasa-sk/NaLLM</p>"},{"location":"fine-tuning/","title":"Fine tuning","text":""},{"location":"fine-tuning/#knowledge-graphs-llms-fine-tuning-vs-retrieval-augmented-generation","title":"Knowledge Graphs &amp; LLMs: Fine-Tuning Vs. Retrieval-Augmented Generation","text":"<p>What are the limitations of LLMs, and how to overcome them</p> <p></p> <p>Midjourney\u2019s idea of a knowledge graph chatbot.</p> <p>This is the second blog post of Neo4j\u2019s NaLLM project. We started this project to explore, develop, and showcase practical uses of these LLMs in conjunction with Neo4j. As part of this project, we will construct and publicly display demonstrations in a GitHub repository, providing an open space for our community to observe, learn, and contribute. Additionally, we are writing about our findings in a blog post. You can find the first and third blog posts here:</p> <ul> <li>Harnessing LLMs With Neo4j</li> <li>Multi-Hop Question Answering</li> </ul> <p>The first wave of hype for Large Language Models (LLMs) came from ChatGPT and similar web-based chatbots, where the models are so good at understanding and generating text that it shocked people, myself included.</p> <p>Many of us logged in and tested its ability to write haikus, motivational letters, or email responses. What became quickly apparent is that LLMs are not only good at generating creative context but also at solving typical natural language processing and other tasks.</p> <p>Shortly after the LLM hype started, people started considering integrating it into their applications. Unfortunately, if you simply develop a wrapper around an LLM API, there is a high chance your application will not be successful as it doesn\u2019t provide additional value.</p> <p>One major problem of LLMs is the so-called knowledge cutoff. The knowledge cutoff term indicates that LLMs are unaware of any events that happened after their training. For example, if you ask ChatGPT about an event in 2023, you will get the following response.</p> <p></p> <p>ChatGPT\u2019s knowledge cutoff date. Image by author.</p> <p>The same problem will occur if you ask an LLM about any event not present in its training dataset. While the knowledge cutoff date is relevant for any publicly available information, the LLM doesn\u2019t have any knowledge about private or confidential information that might be available even before the knowledge cutoff date.</p> <p>For example, most companies have some confidential information that they don\u2019t share publicly but might be interested in having a custom LLM that could answer those questions. On the other hand, a lot of the publicly available information that the LLM is aware of might be already outdated.</p> <p>Therefore, updating and expanding the knowledge of an LLM is highly relevant today.</p> <p>Another problem with LLMs is that they are trained to produce realistic-sounding text, which might not be accurate. Some invalid information is more challenging to spot than others. Especially for missing data, it is very probable that the LLM will make up an answer that sounds convincing but is nonetheless wrong instead of admitting that it lacks the base facts in its training.</p> <p>For example, research or court citations might be easier to verify. A week ago, a lawyer got in trouble for blindly believing the court citations ChatGPT produced.</p> <p>I have also noticed that LLMs will consistently produce assertive, yet false information about any sort of IDs like the WikiData or other identification numbers.</p> <p></p> <p>ChatGPT\u2019s hallucinations. Image by author.</p> <p>Since the response by ChatGPT is assertive, you might expect it to be accurate. However, the given WikiData id points to a farm in England. Therefore, you have to be very careful not to blindly believe everything that LLMs produce. Verifying answers or producing more accurate results from LLMs is another big problem that needs to be solved.</p> <p>Of course, LLMs have other problems, like bias, prompt injection, and others. However, we will not talk about them here. Instead, in this blog post, we will present and focus on the concepts of fine-tuning and retrieval-augmented LLMs and evaluate their pros and cons.</p>"},{"location":"fine-tuning/#supervised-fine-tuning-of-an-llm","title":"Supervised Fine-Tuning of an LLM","text":"<p>Explaining how LLMs are trained is beyond the scope of this blog post. Instead, you can watch this incredible video by Andrej Karpathy to catch up on LLMs and learn about the different phases of LLM training.</p> <p>By fine-tuning an LLM, we refer to the supervised training phase, during which you provide additional question-answer pairs to optimize the performance of the Large Language Model (LLM).</p> <p>Additionally, we have identified two different use cases for fine-tuning an LLM.</p> <p>One use case is fine-tuning a model to update and expand its internal knowledge. \\ In contrast, the other use case is focused on fine-tuning a model for a specific task like text summarization or translating natural language to database queries.</p> <p>First, we will talk about the first use case, where we use fine-tuning techniques to update and expand the internal knowledge of an LLM.</p> <p></p> <p>Supervised fine-tuning flow. Image by author. Icons from Flaticons.</p> <p>Usually, you want to avoid pre-training an LLM as the cost can be upwards of hundreds of thousands and even millions of dollars. A base LLM is pre-trained using a gigantic corpus of text corpus, frequently in the billions or even trillions of tokens.</p> <p>While the number of parameters of an LLM is vital, it is not the only parameter you should consider when selecting a base LLM. Besides the license, you should also consider the bias and toxicity of the pre-training dataset and the base LLM.</p> <p>After you have selected the base LLM, you can start the next step of fine-tuning it. The fine-tuning step is relatively cheap regarding computation cost due to available techniques like the LoRa and QLoRA.</p> <p>However, constructing a training dataset is more complex and can get expensive. If you can not afford a dedicated team of annotators, it seems that the trend is to use an LLM to construct a training dataset to fine-tune your desired LLM (this is really meta).</p> <p>For example, Stanford\u2019s Alpaca training dataset was created using OpenAI\u2019s LLMs. The cost to produce 52 thousand training instructions was about 500 dollars, which is relatively cheap.</p> <p>On the other hand, the Vicuna model was fine-tuned by using the ChatGPT conversations users posted on ShareGPT.com.</p> <p>There is also a relatively fresh project by H2O called WizardLM, which is designed to turn documents into question-answer pairs that can be used to fine-tune an LLM.</p> <p>We haven\u2019t found any recent articles describing how to use a knowledge graph to prepare good question-answer pairs that can be used to fine-tune an LLM.</p> <p>This is an area that we plan to explore during the NaLLM project. We have some ideas for utilizing LLMs to construct question-answer pairs from a knowledge graph context.</p> <p>However, there are a lot of unknowns at the moment. For example, can you provide two different answers to the same question, and the LLM then somehow combines them in its internal knowledge store?</p> <p>Another consideration is that some information in a knowledge graph is not relevant without considering its relationships. Therefore, do we have to pre-define relevant queries, or is there a more generic way to go about it? Or can we use the node-relationship-node patterns representing subject-predicate-object expressions to generate relevant pairs?</p> <p>These are some of the questions we aim to answer in upcoming blog posts.</p> <p>Imagine that you somehow managed to produce a training dataset containing question-answer pairs based on the information stored in your knowledge graph. As a result, the LLM now includes updated knowledge.</p> <p>However, fine-tuning the model didn\u2019t solve the knowledge cutoffs problem since it only pushed the knowledge cutoff to a later date.</p> <p>Therefore, we recommend updating the internal knowledge of an LLM through fine-tuning techniques _only for slowly changing _or updating data. For example, you could use a fine-tuned model to provide tourist information.</p> <p>However, you would run into troubles the second you would want to include special time-dependent (real-time) or personalized promotions in the responses. Similarly, fine-tuned models are not ideal for analytical workflows where you would ask how many new customers the company gained over the last week.</p> <p>At the moment, fine-tuning approaches can help mitigate hallucinations but cannot completely eliminate them. One problem is that the LLMs do not cite their sources when providing answers. Therefore, you have no idea if the answer came from pre-training data, fine-tuning dataset, or was made up by the LLM. Additionally, there might be another possible falsehood source if you use an LLM to create the fine-tuning dataset.</p> <p>Lastly, a fine-tuned model cannot automatically provide different responses depending on the user making the questions. Likewise, there is no concept of access restrictions, meaning that anybody interacting with the LLM has access to all of its information.</p>"},{"location":"fine-tuning/#retrieval-augmented-generation","title":"Retrieval-Augmented Generation","text":"<p>Large language models perform remarkably well in natural language applications like</p> <ul> <li>Text summarization,</li> <li>Extracting relevant information,</li> <li>Disambiguation of entities</li> <li>Translating from one language to another, or even</li> <li>Converting natural language into database queries or scripting code.</li> </ul> <p>Moreover, previously NLP models were most often domain and task-specific, meaning that you would most likely need to train a custom natural language model depending on your use case and domain. However, thanks to the generalization capabilities of LLMs, a single model can be applied to solve various collections of tasks.</p> <p>We have observed quite a strong trend in using retrieval-augmented LLMs, where instead of using LLMs to access its internal knowledge, you use the LLM as a natural language interface to your company\u2019s or private information.</p> <p></p> <p>Retrieval-augmented generation. Image by author. Icons from Flaticons.</p> <p>The retrieval augmented approach uses the LLM to generate an answer based on the additionally provided relevant documents from your data source.</p> <p>Therefore, you don\u2019t rely on internal knowledge of the LLM to produce answers. Instead, the LLM is used only for extracting relevant information from documents you passed in and summarizing it.</p> <p>For example, the ChatGPT plugins can be thought of as a retrieval-augmented approach to LLM applications. The ChatGPT interface with a browsing plugin enabled allows the LLM to search the internet to access up-to-date information and use it to construct the final answer.</p> <p></p> <p>ChatGPT with browsing plugin. Image by author.</p> <p>In this example, ChatGPT was able to answer who won the Oscar for various categories in 2023. But, remember, the cutoff knowledge date for ChatGPT is 2021, so it couldn\u2019t know who won the 2023 Oscars from its internal knowledge. Therefore, it accessed external information through the browsing plugin, which allowed it to answer the question with up-to-date information. Those plugins present an integrated augmentation mechanism inside the OpenAI platform.</p> <p>If you have been watching the LLM space, you might have heard of the LangChain library.</p> <p>The LangChain library can be used to allow LLMs to access real-time information from various sources like Google Search, vector databases, or knowledge graphs. For example, LangChain has added a Cypher Search chain, which converts natural language questions into a Cypher statement, uses it to retrieve information from the Neo4j database, and constructs a final answer based on the provided information.</p> <p>With the Cypher Search chain, an LLM is not only used to construct a final answer but also to translate a natural language question into a Cypher query.</p> <p></p> <p>Cypher search in LangChain. Image by author.</p> <p>Another popular library for retrieval-augmented LLM workflows is LlamaIndex (GPT Index). LlamaIndex is a comprehensive data framework aimed at enhancing the performance of Large Language Models (LLMs) by enabling them to leverage private or custom data.</p> <p>Firstly, LlamaIndex offers data connectors that facilitate the ingestion of a variety of data sources and formats, encompassing everything from APIs, PDFs, and documents to SQL or graph data.</p> <p>This feature allows for an effortless integration of existing data into the LLM. Secondly, it provides efficient mechanisms to structure the ingested data using indices and graphs, ensuring the data is suitably arranged for use with LLMs. In addition, it includes an advanced retrieval and query interface, which enables users to input an LLM prompt and receive back a context-retrieved, knowledge-augmented output.</p> <p>The idea behind retrieval-augmented LLM applications like ChatGPT Plugins and LangChain is to avoid relying on internal LLM knowledge only to generate answers. Instead, LLMs are used to solve tasks like constructing database queries from natural language and constructing answers based on externally provided information or by utilizing plugins/agents for retrieval.</p> <p>The retrieval-augmented approach has some clear advantages over the fine-tuning approach:</p> <ul> <li>The answer can cite its sources of information, which allows you to validate the information and potentially change or update the underlying information based on requirements</li> <li>Hallucinations are more unlikely to occur as you don\u2019t rely on the internal knowledge of an LLM to answer the question and only use information that is provided in the relevant documents</li> <li>Changing, updating, and maintaining the underlying information the LLM uses is easier as you transform the problem from LLM maintenance to a database maintenance, querying and context construction problem</li> <li>Answers can be personalized based on the user context, or their access permission</li> </ul> <p>On the other hand, you should consider the following limitations when using the retrieval-augmented approach:</p> <ul> <li>The answers are only as good as the smart search tool</li> <li>The application needs access to your specific knowledge base, either that be a database or other data stores</li> <li>Completely disregarding the internal knowledge of the language model limits the number of questions that can be answered</li> <li>Sometimes LLMs fail to follow instructions, so there is a risk that the context might be ignored or hallucinations occur if no relevant answer data is found in the context.</li> </ul>"},{"location":"fine-tuning/#summary","title":"Summary","text":"<p>This blog post delves into the limitations of Large Language Models (LLMs), such as</p> <ul> <li>Knowledge cutoff,</li> <li>Hallucinations, and</li> <li>The lack of user customization.</li> </ul> <p>To overcome these, we explored two concepts, namely, fine-tuning and retrieval-augmented use of LLMs.</p> <p>Fine-tuning an LLM involves the supervised training phase, where question-answer pairs are provided to optimize the performance of the LLM. This can be used to update and expand the LLM\u2019s internal knowledge or fine-tune it for a specific task. However, fine-tuning fails to solve the knowledge cutoff issue as it simply pushes the cutoff to a later date. It also cannot fully eliminate hallucinations. Therefore, we recommend using the fine-tuning approach for slowly changing datasets where some hallucinations are allowed. Since fine-tuning LLMs is relatively new, we are eager to learn more about fine-tuning approaches and best practices.</p> <p>The second approach to overcome the limitations of LLMs is the so-called retrieval-augmented generation, where the LLM serves as a natural language interface to access external information, thereby not relying only on its internal knowledge to produce answers. Advantages of the retrieval-augmented approach include source-citing, negligible hallucinations, ease of changing and updating information, and personalization. \\ However, it relies heavily on the intelligent search tool to retrieve relevant information and requires access to the user\u2019s knowledge base. Furthermore, it can only answer queries provided it has the information required to address the question.</p> <p>Keep an eye out for updates from our team as we progress the development of this project, all of which will be openly documented on our GitHub repository.</p>"}]}